{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of test without OpenCL:  0.058000087738 s\n",
      "===============================================================\n",
      "Platform name: AMD Accelerated Parallel Processing\n",
      "Platform profile: FULL_PROFILE\n",
      "Platform vendor: Advanced Micro Devices, Inc.\n",
      "Platform version: OpenCL 2.0 AMD-APP (1800.11)\n",
      "---------------------------------------------------------------\n",
      "Device name: Cayman\n",
      "Device type: GPU\n",
      "Device memory:  2048 MB\n",
      "Device max clock speed: 880 MHz\n",
      "Device compute units: 24\n",
      "Device max work group size: 256\n",
      "Device max work item sizes: [256L, 256L, 256L]\n",
      "Data points: 8388608\n",
      "Workers: 256\n",
      "Preferred work group size multiple: 64\n",
      "Execution time of test: 0.000704111 s\n",
      "Results OK\n",
      "===============================================================\n",
      "Platform name: AMD Accelerated Parallel Processing\n",
      "Platform profile: FULL_PROFILE\n",
      "Platform vendor: Advanced Micro Devices, Inc.\n",
      "Platform version: OpenCL 2.0 AMD-APP (1800.11)\n",
      "---------------------------------------------------------------\n",
      "Device name: Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz\n",
      "Device type: CPU\n",
      "Device memory:  6135 MB\n",
      "Device max clock speed: 3192 MHz\n",
      "Device compute units: 8\n",
      "Device max work group size: 1024\n",
      "Device max work item sizes: [1024L, 1024L, 1024L]\n",
      "Data points:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:95: DeprecationWarning: 'enqueue_read_buffer' has been deprecated in version 2011.1. Please use enqueue_copy() instead.\n",
      "C:\\Anaconda2\\lib\\site-packages\\pyopencl-2016.1-py2.7-win-amd64.egg\\pyopencl\\__init__.py:207: CompilerWarning: Non-empty compiler output encountered. Set the environment variable PYOPENCL_COMPILER_OUTPUT=1 to see more.\n",
      "  \"to see more.\", CompilerWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8388608\n",
      "Workers: 256\n",
      "Preferred work group size multiple: 1\n",
      "Execution time of test: 0.0193318 s\n",
      "Results OK\n",
      "===============================================================\n",
      "Platform name: Intel(R) OpenCL\n",
      "Platform profile: FULL_PROFILE\n",
      "Platform vendor: Intel(R) Corporation\n",
      "Platform version: OpenCL 1.2 \n",
      "---------------------------------------------------------------\n",
      "Device name: Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz\n",
      "Device type: CPU\n",
      "Device memory:  6135 MB\n",
      "Device max clock speed: 2670 MHz\n",
      "Device compute units: 8\n",
      "Device max work group size: 8192\n",
      "Device max work item sizes: [8192L, 8192L, 8192L]\n",
      "Data points: 8388608\n",
      "Workers: 256\n",
      "Preferred work group size multiple: 128\n",
      "Execution time of test: 0.00851488 s\n",
      "Results OK\n",
      "===============================================================\n",
      "Platform name: Experimental OpenCL 2.0 CPU Only Platform\n",
      "Platform profile: FULL_PROFILE\n",
      "Platform vendor: Intel(R) Corporation\n",
      "Platform version: OpenCL 2.0 \n",
      "---------------------------------------------------------------\n",
      "Device name: Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz\n",
      "Device type: CPU\n",
      "Device memory:  6135 MB\n",
      "Device max clock speed: 2670 MHz\n",
      "Device compute units: 8\n",
      "Device max work group size: 8192\n",
      "Device max work item sizes: [8192L, 8192L, 8192L]\n",
      "Data points: 8388608\n",
      "Workers: 256\n",
      "Preferred work group size multiple: 128\n",
      "Execution time of test: 0.00941216 s\n",
      "Results OK\n"
     ]
    }
   ],
   "source": [
    "# example provided by Roger Pau Monn'e\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "import pyopencl as cl\n",
    "import numpy\n",
    "import numpy.linalg as la\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "data_points = 2**23 # ~8 million data points, ~32 MB data\n",
    "workers = 2**8 # 256 workers, play with this to see performance differences\n",
    "               # eg: 2**0 => 1 worker will be non-parallel execution on gpu\n",
    "               # data points must be a multiple of workers\n",
    "\n",
    "a = numpy.random.rand(data_points).astype(numpy.float32)\n",
    "b = numpy.random.rand(data_points).astype(numpy.float32)\n",
    "c_result = numpy.empty_like(a)\n",
    "\n",
    "# Speed in normal CPU usage\n",
    "time1 = time()\n",
    "c_temp = (a+b) # adds each element in a to its corresponding element in b\n",
    "c_temp = c_temp * c_temp # element-wise multiplication\n",
    "c_result = c_temp * (a/2.0) # element-wise half a and multiply\n",
    "time2 = time()\n",
    "\n",
    "print(\"Execution time of test without OpenCL: \", time2 - time1, \"s\")\n",
    "\n",
    "\n",
    "for platform in cl.get_platforms():\n",
    "    for device in platform.get_devices():\n",
    "        print(\"===============================================================\")\n",
    "        print(\"Platform name:\", platform.name)\n",
    "        print(\"Platform profile:\", platform.profile)\n",
    "        print(\"Platform vendor:\", platform.vendor)\n",
    "        print(\"Platform version:\", platform.version)\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print(\"Device name:\", device.name)\n",
    "        print(\"Device type:\", cl.device_type.to_string(device.type))\n",
    "        print(\"Device memory: \", device.global_mem_size//1024//1024, 'MB')\n",
    "        print(\"Device max clock speed:\", device.max_clock_frequency, 'MHz')\n",
    "        print(\"Device compute units:\", device.max_compute_units)\n",
    "        print(\"Device max work group size:\", device.max_work_group_size)\n",
    "        print(\"Device max work item sizes:\", device.max_work_item_sizes)\n",
    "\n",
    "        # Simnple speed test\n",
    "        ctx = cl.Context([device])\n",
    "        queue = cl.CommandQueue(ctx, \n",
    "                properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "\n",
    "        mf = cl.mem_flags\n",
    "        a_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a)\n",
    "        b_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b)\n",
    "        dest_buf = cl.Buffer(ctx, mf.WRITE_ONLY, b.nbytes)\n",
    "\n",
    "        prg = cl.Program(ctx, \"\"\"\n",
    "            __kernel void sum(__global const float *a,\n",
    "            __global const float *b, __global float *c)\n",
    "            {\n",
    "                        int gid = get_global_id(0);\n",
    "                        float a_temp;\n",
    "                        float b_temp;\n",
    "                        float c_temp;\n",
    "\n",
    "                        a_temp = a[gid]; // my a element (by global ref)\n",
    "                        b_temp = b[gid]; // my b element (by global ref)\n",
    "                        \n",
    "                        c_temp = a_temp+b_temp; // sum of my elements\n",
    "                        c_temp = c_temp * c_temp; // product of sums\n",
    "                        c_temp = c_temp * (a_temp/2.0); // times 1/2 my a\n",
    "\n",
    "                        c[gid] = c_temp; // store result in global memory\n",
    "                }\n",
    "                \"\"\").build()\n",
    "\n",
    "        global_size=(data_points,)\n",
    "        local_size=(workers,)\n",
    "        preferred_multiple = cl.Kernel(prg, 'sum').get_work_group_info( \\\n",
    "            cl.kernel_work_group_info.PREFERRED_WORK_GROUP_SIZE_MULTIPLE, \\\n",
    "            device)\n",
    "\n",
    "        print(\"Data points:\", data_points)\n",
    "        print(\"Workers:\", workers)\n",
    "        print(\"Preferred work group size multiple:\", preferred_multiple)\n",
    "\n",
    "        if (workers % preferred_multiple):\n",
    "            print(\"Number of workers not a preferred multiple (%d*N).\" \\\n",
    "                    % (preferred_multiple))\n",
    "            print(\"Performance may be reduced.\")\n",
    "\n",
    "        exec_evt = prg.sum(queue, global_size, local_size, a_buf, b_buf, dest_buf)\n",
    "        exec_evt.wait()\n",
    "        elapsed = 1e-9*(exec_evt.profile.end - exec_evt.profile.start)\n",
    "\n",
    "        print(\"Execution time of test: %g s\" % elapsed)\n",
    "\n",
    "        c = numpy.empty_like(a)\n",
    "        cl.enqueue_read_buffer(queue, dest_buf, c).wait()\n",
    "        equal = numpy.all( c == c_result)\n",
    "\n",
    "        if not equal:\n",
    "                print(\"Results doesn't match!!\")\n",
    "        else:\n",
    "                print(\"Results OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
